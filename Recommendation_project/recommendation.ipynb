{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a93eabc-5a52-425c-8af2-c059c094fcb0",
   "metadata": {},
   "source": [
    "# Imports and dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7481239e-af13-4ab2-921a-8b342a07525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\christian ishimwe\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\christian ishimwe\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/72.0 MB 1.2 MB/s eta 0:01:00\n",
      "    --------------------------------------- 1.3/72.0 MB 2.0 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 2.4/72.0 MB 2.8 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 3.7/72.0 MB 3.5 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 4.7/72.0 MB 3.9 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 6.0/72.0 MB 4.2 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 7.3/72.0 MB 4.5 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 7.6/72.0 MB 4.5 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 7.6/72.0 MB 4.5 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 10.2/72.0 MB 4.7 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 10.5/72.0 MB 4.3 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 11.0/72.0 MB 4.1 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 11.5/72.0 MB 4.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 11.5/72.0 MB 4.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 12.1/72.0 MB 3.6 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 13.1/72.0 MB 3.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 14.2/72.0 MB 3.7 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 14.9/72.0 MB 3.7 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 16.0/72.0 MB 3.8 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 17.0/72.0 MB 3.8 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 18.1/72.0 MB 3.9 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 18.4/72.0 MB 3.9 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 18.4/72.0 MB 3.9 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 20.4/72.0 MB 3.9 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 20.7/72.0 MB 3.8 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 21.5/72.0 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 22.0/72.0 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 22.8/72.0 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 23.1/72.0 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 23.1/72.0 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 24.6/72.0 MB 3.7 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 25.4/72.0 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 26.0/72.0 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 26.0/72.0 MB 3.6 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 27.5/72.0 MB 3.6 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 27.5/72.0 MB 3.6 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 28.0/72.0 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 28.8/72.0 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 29.6/72.0 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 30.4/72.0 MB 3.5 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 31.2/72.0 MB 3.5 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 32.0/72.0 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 32.8/72.0 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 33.6/72.0 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 34.3/72.0 MB 3.5 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 35.1/72.0 MB 3.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 36.2/72.0 MB 3.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 37.0/72.0 MB 3.5 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 37.5/72.0 MB 3.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 38.5/72.0 MB 3.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 39.3/72.0 MB 3.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 40.1/72.0 MB 3.6 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 41.2/72.0 MB 3.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 41.9/72.0 MB 3.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 42.7/72.0 MB 3.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 43.8/72.0 MB 3.6 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 44.6/72.0 MB 3.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 45.4/72.0 MB 3.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 46.1/72.0 MB 3.6 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 47.2/72.0 MB 3.6 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 48.0/72.0 MB 3.6 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 49.0/72.0 MB 3.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 49.8/72.0 MB 3.7 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 50.6/72.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 51.4/72.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 52.2/72.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 53.0/72.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 54.0/72.0 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 54.8/72.0 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 55.6/72.0 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 56.4/72.0 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 57.1/72.0 MB 3.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 58.2/72.0 MB 3.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 59.0/72.0 MB 3.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 60.0/72.0 MB 3.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 60.6/72.0 MB 3.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 61.6/72.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 62.4/72.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 63.4/72.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 64.2/72.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 65.0/72.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 65.8/72.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 66.6/72.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 67.6/72.0 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 68.4/72.0 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 69.5/72.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  70.5/72.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.3/72.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.1\n"
     ]
    }
   ],
   "source": [
    "# Install xgboost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02afbc15-85b2-452b-bfd4-cb1acb5bd779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded:\n",
      "Customers: (187, 10) | Social: (155, 5) | Transactions: (150, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# files\n",
    "customers   = pd.read_csv('data/cleaned_customer_data.csv')\n",
    "social      = pd.read_excel('data/customer_social_profiles.xlsx')\n",
    "transactions = pd.read_excel('data/customer_transactions.xlsx')\n",
    "\n",
    "print(\"Files loaded:\")\n",
    "print(f\"Customers: {customers.shape} | Social: {social.shape} | Transactions: {transactions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de640ba3-5884-4fb1-a1db-b74c86854f30",
   "metadata": {},
   "source": [
    "# Data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "141a50ff-3c98-4b60-884a-0e60ccafcda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUSTOMERS (first 3 rows) ===\n",
      "   customer_id_clean  transaction_id purchase_date product_category  \\\n",
      "0                151            1001    2024-01-01           Sports   \n",
      "1                151            1001    2024-01-01           Sports   \n",
      "2                192            1002    2024-01-02      Electronics   \n",
      "\n",
      "   purchase_amount  customer_rating social_media_platform  engagement_score  \\\n",
      "0              408              2.3                TikTok                61   \n",
      "1              408              2.3               Twitter                72   \n",
      "2              332              4.2             Instagram                60   \n",
      "\n",
      "   purchase_interest_score review_sentiment  \n",
      "0                      1.3          Neutral  \n",
      "1                      1.6          Neutral  \n",
      "2                      4.3         Positive  \n",
      "\n",
      "Columns: ['customer_id_clean', 'transaction_id', 'purchase_date', 'product_category', 'purchase_amount', 'customer_rating', 'social_media_platform', 'engagement_score', 'purchase_interest_score', 'review_sentiment']\n",
      "\n",
      "=== SOCIAL PROFILES (first 3 rows) ===\n",
      "  customer_id_new social_media_platform  engagement_score  \\\n",
      "0            A178              LinkedIn                74   \n",
      "1            A190               Twitter                82   \n",
      "2            A150              Facebook                96   \n",
      "\n",
      "   purchase_interest_score review_sentiment  \n",
      "0                      4.9         Positive  \n",
      "1                      4.8          Neutral  \n",
      "2                      1.6         Positive  \n",
      "\n",
      "Columns: ['customer_id_new', 'social_media_platform', 'engagement_score', 'purchase_interest_score', 'review_sentiment']\n",
      "\n",
      "=== TRANSACTIONS (first 3 rows) ===\n",
      "   customer_id_legacy  transaction_id  purchase_amount purchase_date  \\\n",
      "0                 151            1001              408    2024-01-01   \n",
      "1                 192            1002              332    2024-01-02   \n",
      "2                 114            1003              442    2024-01-03   \n",
      "\n",
      "  product_category  customer_rating  \n",
      "0           Sports              2.3  \n",
      "1      Electronics              4.2  \n",
      "2      Electronics              2.1  \n",
      "\n",
      "Columns: ['customer_id_legacy', 'transaction_id', 'purchase_amount', 'purchase_date', 'product_category', 'customer_rating']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "customers   = pd.read_csv('data/cleaned_customer_data.csv')\n",
    "social      = pd.read_excel('data/customer_social_profiles.xlsx')\n",
    "transactions = pd.read_excel('data/customer_transactions.xlsx')\n",
    "\n",
    "print(\"\\n=== CUSTOMERS (first 3 rows) ===\")\n",
    "print(customers.head(3))\n",
    "print(\"\\nColumns:\", customers.columns.tolist())\n",
    "\n",
    "print(\"\\n=== SOCIAL PROFILES (first 3 rows) ===\")\n",
    "print(social.head(3))\n",
    "print(\"\\nColumns:\", social.columns.tolist())\n",
    "\n",
    "print(\"\\n=== TRANSACTIONS (first 3 rows) ===\")\n",
    "print(transactions.head(3))\n",
    "print(\"\\nColumns:\", transactions.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d036caa-1bac-4e73-a5af-39b653510397",
   "metadata": {},
   "source": [
    "# Auto detect merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1872b85e-4dd0-4603-be29-75caad5f301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected key (transactions ↔ customers): None\n",
      "Detected key (transactions ↔ social):    None\n"
     ]
    }
   ],
   "source": [
    "# List of common ID column names\n",
    "id_candidates = [\n",
    "    'customer_id', 'CustomerID', 'cust_id', 'user_id', 'id', 'customerID', 'CustomerId',\n",
    "    'customer_key', 'cust_key', 'user_key', 'client_id', 'ClientID', 'account_id'\n",
    "]\n",
    "\n",
    "def find_merge_key(df1, df2, candidates=id_candidates):\n",
    "    for col in candidates:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# Detect keys\n",
    "key_tc = find_merge_key(transactions, customers)\n",
    "key_ts = find_merge_key(transactions, social)\n",
    "\n",
    "print(f\"Detected key (transactions ↔ customers): {key_tc}\")\n",
    "print(f\"Detected key (transactions ↔ social):    {key_ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf9136-9f7f-4495-8162-073b3ca7dbb5",
   "metadata": {},
   "source": [
    "# Safe merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e509d06-7a1e-462d-8fd1-5408d333c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUSTOMERS columns  : ['customer_id_clean', 'transaction_id', 'purchase_date', 'product_category', 'purchase_amount', 'customer_rating', 'social_media_platform', 'engagement_score', 'purchase_interest_score', 'review_sentiment']\n",
      "SOCIAL columns     : ['customer_id_new', 'social_media_platform', 'engagement_score', 'purchase_interest_score', 'review_sentiment']\n",
      "TRANSACTIONS columns: ['customer_id_legacy', 'transaction_id', 'purchase_amount', 'purchase_date', 'product_category', 'customer_rating']\n",
      "No common key with customers – continuing without customer data\n",
      "No common key with social – continuing without social data\n",
      "\n",
      "Merged dataset shape: (150, 6)\n",
      "   customer_id_legacy  transaction_id  purchase_amount purchase_date  \\\n",
      "0                 151            1001              408    2024-01-01   \n",
      "1                 192            1002              332    2024-01-02   \n",
      "2                 114            1003              442    2024-01-03   \n",
      "3                 171            1004              256    2024-01-04   \n",
      "4                 160            1005               64    2024-01-05   \n",
      "\n",
      "  product_category  customer_rating  \n",
      "0           Sports              2.3  \n",
      "1      Electronics              4.2  \n",
      "2      Electronics              2.1  \n",
      "3         Clothing              2.8  \n",
      "4         Clothing              1.3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the files (in case they changed)\n",
    "customers   = pd.read_csv('data/cleaned_customer_data.csv')\n",
    "social      = pd.read_excel('data/customer_social_profiles.xlsx')\n",
    "transactions = pd.read_excel('data/customer_transactions.xlsx')\n",
    "\n",
    "print(\"\\nCUSTOMERS columns  :\", customers.columns.tolist())\n",
    "print(\"SOCIAL columns     :\", social.columns.tolist())\n",
    "print(\"TRANSACTIONS columns:\", transactions.columns.tolist())\n",
    "\n",
    "candidates = [\n",
    "    'customer_id', 'CustomerID', 'cust_id', 'user_id', 'id', 'customerID',\n",
    "    'CustomerId', 'client_id', 'ClientID', 'account_id', 'cust_key',\n",
    "    'customer_key', 'user_key', 'Customer_ID', 'CustID'\n",
    "]\n",
    "\n",
    "def find_common(df1, df2, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df1.columns and c in df2.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "key_tc = find_common(transactions, customers, candidates)\n",
    "key_ts = find_common(transactions, social, candidates)\n",
    "\n",
    "df = transactions.copy()                     # start with transactions\n",
    "\n",
    "if key_tc:\n",
    "    df = df.merge(customers, on=key_tc, how='left')\n",
    "    print(f\"Customers merged on '{key_tc}'\")\n",
    "else:\n",
    "    print(\"No common key with customers – continuing without customer data\")\n",
    "\n",
    "if key_ts:\n",
    "    df = df.merge(social, on=key_ts, how='left')\n",
    "    print(f\"Social profiles merged on '{key_ts}'\")\n",
    "else:\n",
    "    print(\"No common key with social – continuing without social data\")\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6c2fc-dfa9-4cf6-81fc-3fdca3b50da5",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9598778-6a83-4012-9994-e6143b6283a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL COLUMNS: ['customer_id_legacy', 'transaction_id', 'purchase_amount', 'purchase_date', 'product_category_x', 'customer_rating', 'product_id', 'user_total_spend', 'user_num_orders', 'user_last_purchase', 'user_avg_rating', 'product_price_mean', 'product_category_y', 'product_avg_rating', 'days_since_last_purchase', 'clicked', 'purchased', 'product_category']\n",
      "Using: customer='customer_id_legacy', product='product_id'\n",
      "Category column set to: 'product_category'\n",
      "Latest date: 2024-05-29\n",
      "\n",
      "FEATURE ENGINEERING DONE! → (150, 18)\n",
      "Sample:\n",
      "   customer_id_legacy        product_id  purchase_amount  \\\n",
      "0                 151       1001_Sports              408   \n",
      "1                 192  1002_Electronics              332   \n",
      "2                 114  1003_Electronics              442   \n",
      "\n",
      "   days_since_last_purchase  user_total_spend product_category  \n",
      "0                       149               408           Sports  \n",
      "1                        72               823      Electronics  \n",
      "2                        44              1034      Electronics  \n"
     ]
    }
   ],
   "source": [
    "# 4. FEATURE ENGINEERING – FINAL & 100% WORKING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ALL COLUMNS:\", df.columns.tolist())\n",
    "\n",
    "\n",
    "# 1. KEYS (use existing ones)\n",
    "\n",
    "key_tc   = 'customer_id_legacy'      # customer ID\n",
    "prod_col = 'product_id'              # already exists!\n",
    "price_col = 'purchase_amount'\n",
    "date_col  = 'purchase_date'\n",
    "\n",
    "print(f\"Using: customer='{key_tc}', product='{prod_col}'\")\n",
    "\n",
    "\n",
    "# 2. CLEAN UP DUPLICATE product_category (x and y)\n",
    "\n",
    "# Use the one from original data: 'product_category_x'\n",
    "if 'product_category_x' in df.columns:\n",
    "    df['product_category'] = df['product_category_x']\n",
    "elif 'product_category_y' in df.columns:\n",
    "    df['product_category'] = df['product_category_y']\n",
    "else:\n",
    "    df['product_category'] = 'unknown'\n",
    "\n",
    "print(f\"Category column set to: 'product_category'\")\n",
    "\n",
    "# 3. DATE\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "df = df.dropna(subset=[date_col]).copy()\n",
    "latest_date = df[date_col].max()\n",
    "print(f\"Latest date: {latest_date.date()}\")\n",
    "\n",
    "# 4. FINAL FEATURES (already exist – just verify)\n",
    "required = ['user_total_spend', 'days_since_last_purchase']\n",
    "for col in required:\n",
    "    if col not in df.columns:\n",
    "        if col == 'user_total_spend':\n",
    "            df[col] = df.groupby(key_tc)['purchase_amount'].transform('sum')\n",
    "        elif col == 'days_since_last_purchase':\n",
    "            df['user_last_purchase'] = df.groupby(key_tc)[date_col].transform('max')\n",
    "            df[col] = (latest_date - df['user_last_purchase']).dt.days\n",
    "\n",
    "df['clicked'] = 1\n",
    "df['purchased'] = 1\n",
    "\n",
    "# 5. SAMPLE PRINT (safe)\n",
    "sample_cols = [key_tc, prod_col, price_col, 'days_since_last_purchase', 'user_total_spend', 'product_category']\n",
    "print(f\"\\nFEATURE ENGINEERING DONE! → {df.shape}\")\n",
    "print(\"Sample:\")\n",
    "print(df[sample_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc84025-ea59-423c-b06b-e6ea842a4c02",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dd092c8-93d0-4501-8cf7-f9ac2ba2c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL DATASET: (600, 8)\n",
      "Class balance:\n",
      "purchased\n",
      "0    0.75\n",
      "1    0.25\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample:\n",
      "   customer_id_legacy        product_id  product_price_mean  user_total_spend  \\\n",
      "0                 151       1001_Sports               408.0               408   \n",
      "1                 192  1002_Electronics               332.0               823   \n",
      "2                 114  1003_Electronics               442.0              1034   \n",
      "\n",
      "   days_since_last_purchase  clicked  category_id  purchased  \n",
      "0                       149        1            4          1  \n",
      "1                        72        1            2          1  \n",
      "2                        44        1            2          1  \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 5. NEGATIVE SAMPLING – FINAL, 100 % WORKING\n",
    "# -------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. ENSURE product_category exists\n",
    "if 'product_category' not in df.columns:\n",
    "    if 'product_category_y' in df.columns:\n",
    "        df['product_category'] = df['product_category_y']\n",
    "    elif 'product_category_x' in df.columns:\n",
    "        df['product_category'] = df['product_category_x']\n",
    "    else:\n",
    "        df['product_category'] = 'unknown'\n",
    "\n",
    "# 2. SHARED category → id mapping\n",
    "all_categories = pd.concat([df['product_category'], pd.Series(['unknown'])]).astype('category')\n",
    "cat_to_id = dict(zip(all_categories.cat.categories, range(len(all_categories.cat.categories))))\n",
    "\n",
    "# 3. NEGATIVE SAMPLES\n",
    "purchased_pairs = set(zip(df[key_tc], df[prod_col]))\n",
    "n_neg = len(df) * 3\n",
    "users = df[key_tc].unique()\n",
    "products = df[prod_col].unique()\n",
    "\n",
    "np.random.seed(42)\n",
    "neg_samples = []\n",
    "while len(neg_samples) < n_neg:\n",
    "    u = np.random.choice(users)\n",
    "    p = np.random.choice(products)\n",
    "    if (u, p) not in purchased_pairs:\n",
    "        neg_samples.append({key_tc: u, prod_col: p, 'purchased': 0})\n",
    "\n",
    "neg_df = pd.DataFrame(neg_samples)\n",
    "\n",
    "# user stats\n",
    "neg_df = neg_df.merge(\n",
    "    df[[key_tc, 'user_total_spend', 'user_last_purchase']].drop_duplicates(),\n",
    "    on=key_tc, how='left'\n",
    ")\n",
    "\n",
    "# product stats\n",
    "neg_df = neg_df.merge(\n",
    "    df[[prod_col, 'product_price_mean', 'product_category']].drop_duplicates(),\n",
    "    on=prod_col, how='left'\n",
    ")\n",
    "\n",
    "# fill missing\n",
    "neg_df['clicked'] = 0\n",
    "neg_df['days_since_last_purchase'] = (latest_date - neg_df['user_last_purchase']).dt.days\n",
    "neg_df['user_total_spend'] = neg_df['user_total_spend'].fillna(0)\n",
    "neg_df['product_price_mean'] = neg_df['product_price_mean'].fillna(df[price_col].mean())\n",
    "neg_df['product_category'] = neg_df['product_category'].fillna('unknown')\n",
    "neg_df['category_id'] = neg_df['product_category'].map(cat_to_id)\n",
    "\n",
    "# FIX: Remove duplicate columns\n",
    "neg_df = neg_df.loc[:, ~neg_df.columns.duplicated()]\n",
    "neg_df = neg_df.reset_index(drop=True)\n",
    "\n",
    "# 4. POSITIVE SAMPLES\n",
    "pos_df = df[[key_tc, prod_col, price_col,\n",
    "            'product_price_mean', 'user_total_spend',\n",
    "            'days_since_last_purchase', 'clicked']].copy()\n",
    "\n",
    "pos_df['purchased'] = 1\n",
    "pos_df = pos_df.merge(\n",
    "    df[[prod_col, 'product_category']].drop_duplicates(),\n",
    "    on=prod_col, how='left'\n",
    ")\n",
    "pos_df['product_category'] = pos_df['product_category'].fillna('unknown')\n",
    "pos_df['category_id'] = pos_df['product_category'].map(cat_to_id)\n",
    "pos_df = pos_df.rename(columns={price_col: 'product_price_mean'})\n",
    "\n",
    "# FIX: Remove duplicates & reset\n",
    "pos_df = pos_df.loc[:, ~pos_df.columns.duplicated()]\n",
    "pos_df = pos_df.reset_index(drop=True)\n",
    "\n",
    "# 5. FINAL DATASET\n",
    "common_cols = [\n",
    "    key_tc, prod_col, 'product_price_mean', 'user_total_spend',\n",
    "    'days_since_last_purchase', 'clicked', 'category_id', 'purchased'\n",
    "]\n",
    "\n",
    "final_df = pd.concat([pos_df[common_cols], neg_df[common_cols]], ignore_index=True)\n",
    "\n",
    "print(f\"\\nFINAL DATASET: {final_df.shape}\")\n",
    "print(\"Class balance:\")\n",
    "print(final_df['purchased'].value_counts(normalize=True).round(3))\n",
    "print(\"\\nSample:\")\n",
    "print(final_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bffb4-a67e-458f-8498-b05a06e3d75c",
   "metadata": {},
   "source": [
    "# Model training/ the best one \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84794abc-09f0-425f-a651-d7672cb2fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL COMPARISON\n",
      "       Model  F1  LogLoss\n",
      "RandomForest 1.0    0.057\n",
      "    Logistic 1.0    0.032\n",
      "     XGBoost 1.0    0.003\n",
      "\n",
      "BEST MODEL: RandomForest\n",
      "Saved: best_model_RandomForest.pkl | preprocessor.pkl | recommendation_artifacts.pkl\n",
      "\n",
      "TOP 5 RECOMMENDATIONS FOR CUSTOMER 151:\n",
      "       Category  Price  Score\n",
      "99     Clothing   64.0  0.053\n",
      "78       Sports   91.0  0.043\n",
      "25       Sports   88.0  0.040\n",
      "30       Sports   85.0  0.038\n",
      "87  Electronics  228.0  0.032\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 6. TRAIN, COMPARE & RECOMMEND (Random Forest, Logistic, XGBoost)\n",
    "# -------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Features\n",
    "# -------------------------------\n",
    "num_features    = ['product_price_mean', 'user_total_spend', 'days_since_last_purchase']\n",
    "cat_features    = ['category_id']\n",
    "binary_features = ['clicked']\n",
    "\n",
    "X = final_df[num_features + cat_features + binary_features]\n",
    "y = final_df['purchased']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Shared Preprocessor\n",
    "# -------------------------------\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('bin', 'passthrough', binary_features)\n",
    "])\n",
    "\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_val_pre   = preprocessor.transform(X_val)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Train 3 Models\n",
    "# -------------------------------\n",
    "models = {}\n",
    "\n",
    "# Random Forest\n",
    "models['RandomForest'] = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=6, class_weight='balanced',\n",
    "    n_jobs=-1, random_state=42)\n",
    "models['RandomForest'].fit(X_train_pre, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "models['Logistic'] = LogisticRegression(\n",
    "    max_iter=1000, class_weight='balanced', n_jobs=-1, random_state=42)\n",
    "models['Logistic'].fit(X_train_pre, y_train)\n",
    "\n",
    "# XGBoost\n",
    "scale_pos = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "models['XGBoost'] = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos, eval_metric='logloss',\n",
    "    n_jobs=-1, random_state=42, verbosity=0)\n",
    "models['XGBoost'].fit(X_train_pre, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Compare Models\n",
    "# -------------------------------\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_val_pre)\n",
    "    y_prob = model.predict_proba(X_val_pre)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'F1'   : round(f1_score(y_val, y_pred), 3),\n",
    "        'LogLoss': round(log_loss(y_val, y_prob), 3)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('F1', ascending=False).reset_index(drop=True)\n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save Best Model\n",
    "# -------------------------------\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_name]\n",
    "\n",
    "joblib.dump(best_model,   f'best_model_{best_name}.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "with open('recommendation_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'cat_to_id'   : cat_to_id,\n",
    "        'latest_date' : latest_date,\n",
    "        'key_tc'      : key_tc,\n",
    "        'prod_col'    : prod_col,\n",
    "        'price_col'   : price_col,\n",
    "        'df_snapshot' : df[[key_tc, prod_col, 'product_category',\n",
    "                           'product_price_mean', 'user_total_spend',\n",
    "                           'user_last_purchase']].drop_duplicates()\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_name}\")\n",
    "print(f\"Saved: best_model_{best_name}.pkl | preprocessor.pkl | recommendation_artifacts.pkl\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. RECOMMEND FUNCTION (uses best model)\n",
    "# -------------------------------\n",
    "def recommend(customer_id, n=5, model=best_model, top_n_cand=100):\n",
    "    user_row = df[df[key_tc] == customer_id][[key_tc,\n",
    "                'user_total_spend', 'user_last_purchase']].head(1)\n",
    "    if user_row.empty:\n",
    "        return f\"Customer {customer_id} not found.\"\n",
    "\n",
    "    purchased = set(df[df[key_tc] == customer_id][prod_col])\n",
    "    cand = df[~df[prod_col].isin(purchased)][[prod_col,\n",
    "                'product_price_mean', 'product_category']].drop_duplicates()\n",
    "\n",
    "    if cand.empty:\n",
    "        return \"No new products.\"\n",
    "\n",
    "    cand = cand.sample(min(top_n_cand, len(cand)), random_state=42).copy()\n",
    "    cand = cand.merge(user_row, how='cross')\n",
    "    cand['clicked'] = 0\n",
    "    cand['days_since_last_purchase'] = (latest_date - cand['user_last_purchase']).dt.days\n",
    "    cand['category_id'] = cand['product_category'].map(cat_to_id)\n",
    "\n",
    "    X_cand = preprocessor.transform(cand[num_features + cat_features + binary_features])\n",
    "    cand['score'] = model.predict_proba(X_cand)[:, 1]\n",
    "\n",
    "    out = cand.nlargest(n, 'score')[['product_category', 'product_price_mean', 'score']]\n",
    "    out.columns = ['Category', 'Price', 'Score']\n",
    "    return out.round(3)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. TEST RECOMMENDATION\n",
    "# -------------------------------\n",
    "test_cust = df[key_tc].iloc[0]\n",
    "print(f\"\\nTOP 5 RECOMMENDATIONS FOR CUSTOMER {test_cust}:\")\n",
    "print(recommend(test_cust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b551c-596c-4d03-992b-54f5dc05bd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
